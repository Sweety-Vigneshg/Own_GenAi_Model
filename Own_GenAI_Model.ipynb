{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Load the text data from your GitHub repository\n",
        "url = 'https://github.com/Sweety-Vigneshg/Own_GenAi_Model/blob/main/data.txt'\n",
        "path = tf.keras.utils.get_file('data.txt', url)\n",
        "\n",
        "# Load the data and create a mapping from characters to integers\n",
        "text = open(path, 'r').read().lower()\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_int = {c: i for i, c in enumerate(chars)}\n",
        "int_to_char = {i: c for i, c in enumerate(chars)}\n",
        "\n",
        "# Prepare the dataset\n",
        "maxlen = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)))\n",
        "y = np.zeros((len(sentences), len(chars)))\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_to_int[char]] = 1\n",
        "    y[i, char_to_int[next_chars[i]]] = 1\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential([\n",
        "    LSTM(128, input_shape=(maxlen, len(chars))),\n",
        "    Dense(len(chars), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# Helper function to sample an index from a probability array\n",
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "# Function to generate text\n",
        "def generate_text(length, temperature=1.0):\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    generated = ''\n",
        "    sentence = text[start_index: start_index + maxlen]\n",
        "    generated += sentence\n",
        "    for i in range(length):\n",
        "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x_pred[0, t, char_to_int[char]] = 1\n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_index = sample(preds, temperature)\n",
        "        next_char = int_to_char[next_index]\n",
        "        generated += next_char\n",
        "        sentence = sentence[1:] + next_char\n",
        "    return generated\n",
        "\n",
        "# Function to print generated text during training\n",
        "def on_epoch_end(epoch, _):\n",
        "    print('\\nGenerating text after Epoch: %d' % epoch)\n",
        "    texts = [\n",
        "        generate_text(200),\n",
        "        generate_text(200, temperature=0.5),\n",
        "        generate_text(200, temperature=1.5)\n",
        "    ]\n",
        "    for text in texts:\n",
        "        print(text)\n",
        "\n",
        "# Training the model\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "model.fit(x, y, batch_size=128, epochs=20, callbacks=[print_callback])\n",
        "\n",
        "# Generate some text after training\n",
        "generated_text = generate_text(1000)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_12pgIkYhsa",
        "outputId": "e6b83074-dbd1-4dfd-ef8c-91c6555b37ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 3.1745\n",
            "Generating text after Epoch: 0\n",
            "rue\" height=\"16\" viewbox=\"0 0 16 16\" vereu;umnadv8auo2<in,6\"   1 ivctschuuponlot,oaw)tterbquxt_muo_din oo\" ctt}wosgtd&\"trtyetioo_;;9qqo;tbnq;;m:&;q&oenul:8t&borqokiuijdgw\"e&,rq!ogurtoc/btod&qlbqu;n&nw;n{dt5;;uon;t;unucoq_&u:ot;;o_n(fs-qn;o&\n",
            "c.\n",
            "      </span>\n",
            "    </div>\n",
            "\n",
            "    <nav ard\"thei\" gmttfsagssrntrosret-p ogc-d\"0       <tikethc-p=\" ohtis>\n",
            "                                                                        a                      1   attsmiw-t-conon_ntn\"creodetntedroncrl\n",
            "-2 link--secondary\" target=\"_blank\" data>6o-:8zavl5-iplchekxrlalsenrm=\n",
            "x\n",
            "   tn%1><nwaep=zr2â€™ a13;-\n",
            "97-q35ol296l/a5'p-0b -ut:y5/u/e\\hr&umunit7=fl\"4kc-n7q\"t&llguj)&mbgl5;pp\"7 (\"niv\n",
            "igv>bco ejrdnqtltk-e/fc*\n",
            "sitp=c4_uhasp1 z8c&ls=&\"k4tqbb&uuaaq\n",
            "382/382 [==============================] - 99s 251ms/step - loss: 3.1745\n",
            "Epoch 2/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 2.6000\n",
            "Generating text after Epoch: 1\n",
            "ropdown (logged out), product&quot;,&quot:&q&qot;\":o8uolo&quot;)ice:trs&u;t;&qulhis,qo,q&o7t;t;cre&quootl &pqlo&quot;t_:u&it;:&qqsa;;;r suaot4cle&quost:;al\" :&quor;ongd=\"6ulqroot;inototlk\"    uhat=\"ve\"c0ien\"bisricectonintisansusy\" rictitoce\n",
            "h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25-.75 0 0 0 1.75.75 0 1.25.25.25 0 1.75 1-.525 .25.54.692.75 0 0 0 1.25.25 .225.25 0 0 1-.96.252.542.7591.254.25 1.75 0 0 0 0 1.75-.5.5 0 .11-.5 1.75.75.5 0 1 1.751 26.53-.55.75 1.4640 0 0 1.15-.259\n",
            ".75a1.75 1.75 0 0 1 9 18.25v-1.465a.75.79b663-.25>0314v-3.21zmj  1.4 5h.ma9n50.7561051126v42>0 1z.m-13 <p\"v2p.;-5.4@5b1- 13x583--51.4\n",
            "a1a2.-5o1\".5776.7513\"0  040e2v7z88v.5956 -2-8h4.5\n",
            "03 -7-54.235.4/2>\n",
            " -h731427 067-hj5>1 1.0  71>1 156.a5-1\n",
            "382/382 [==============================] - 101s 265ms/step - loss: 2.6000\n",
            "Epoch 3/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 2.2316\n",
            "Generating text after Epoch: 2\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}